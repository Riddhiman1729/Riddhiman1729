# Riddhiman Bhattacharya

## Bio
Hi, I am Riddhiman. I am currently an AI Health Fellow in Duke University working in the applications of Reinforcement Learning and Machine Learning for biomedical data. Previously, I was a Postdoctoral Researcher at Daniels School of Business, Purdue University working in Statistics and Reinforcement Learning. I obtained my PhD from the University of Minnesota, under the guidance of Professor Tiefeng Jiang. I have also worked with Professor Galin Jones, Professor Wei Sun, Professor Thanh Nguyen and Professor Mohit Tawarmalani. I am interested in Fast Sampling, Stochastic Optimization, Markov Chain Monte Carlo, Reinforcement Learning, MDPs, Bayesian Modeling.

## Accepted/Published Works
### Explicit Constraints on the Geometric Rate of Convergence of Random Walk Metropolis-Hastings
This work entails deriving explicit convergence bounds and conditions of geometric ergodicity of the Random Walk Metropolis algorithm. This project is accepted at "Bernoulli"-DOI: 10.3150/24-BEJ1796. 

- [Link to Project](https://arxiv.org/abs/2307.11644)

### Active Learning for Fair and Stable Online Allocations
In this work we examine resource allocation in an online setting using active feedback. We formulate the problem as a stochastic optimization problem with bandit feedback and establish sublinear regret bounds in the time horizon. Finally we show experimental results which match our theoretical findings. This project has been accepted in EC 2024 in the conference to journal submission.

- [Link to Project](https://dl.acm.org/doi/10.1145/3670865.3673617)

### Gradient-based Discrete Sampling with Automatic Cyclical Scheduling
 We propose an automatic cyclical scheduling, designed for efficient and accurate sampling in multimodal discrete distributions. Our method contains three key components: (1) a cyclical step size schedule where large steps discover new modes and small steps exploit each mode; (2) a cyclical balancing schedule, ensuring "balanced" proposals for given step sizes and high efficiency of the Markov chain; and (3) an automatic tuning scheme for adjusting the hyperparameters in the cyclical schedules, allowing adaptability across diverse datasets with minimal tuning. We prove the non-asymptotic convergence and inference guarantee for our method in general discrete distributions. Extensive experiments demonstrate the superiority of our method in sampling complex multimodal discrete distributions.

- [Link to Project](https://openreview.net/forum?id=4syq5cgwA2&referrer=%5Bthe%20profile%20of%20Ruqi%20Zhang%5D(%2Fprofile%3Fid%3D~Ruqi_Zhang1))

## Submitted Works

### Non-Asymptotic Analysis of Online Multiplicative Stochastic Gradient Descent
This work is on Stochastic Gradient Descent with Mutiplicative noise and its continuous time behaviour. This work is under minor revision in the Journal of Applied Probability.

- [Link to Project](https://arxiv.org/abs/2112.07110)


### Fast Sampling and Inference via Preconditioned Langevin Dynamics
This work is on fast sampling algorithms with preconditioning. This has been submitted to "JRSSB".

- [Link to Project](https://arxiv.org/abs/2310.07542)


### Active Learning for Fair and Stable Online Allocations
In this work we examine resource allocation in an online setting using active feedback. We formulate the problem as a stochastic optimization problem with bandit feedback and establish sublinear regret bounds in the time horizon. We analyze fairness using different metrics including-max-min, envy and eventually general stability.  Finally we show experimental results which match our theoretical findings. This project has been submitted to MATH-OR.

- [Link to Project](https://arxiv.org/abs/2310.07542)

### Entropy-Guided Sampling of Flat Modes in Discrete Spaces
This paper explores sampling from flat modes in discrete spaces. We propose the Entropic Discrete Langevin Proposal (EDLP) which integrates the discrete sampling variable with a continuous auxiliary variable under a joint distribution framework. In particular, by adapting the Discrete Langevin Proposal (DLP) to leverage local entropy, EDLP enables efficient exploration of flat regions. Backed by rigorous theoretical guarantees, our approach consistently outperforms traditional methods across tasks necessitating sampling from flat basins such as Ising models, restricted Boltzmann machines, and binary neural networks. This project is submitted to ICML 2025.

## Skills
- Latex, R, Python, MATLAB, SAS, Pyspark, SparkR, SparkSQL

## Contact

- Email: riddhiman.bhattacharya@duke.edu




